{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import numpy as np\n",
    "from LOCO_TLPA import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw_model = KeyBERT()\n",
    "# kw_model.extract_keywords(docs=[txt], use_maxsum=True, vectorizer=KeyphraseCountVectorizer(), stop_words=\"english\", top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename=\"preprocessing.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/enron/raw.csv\",\n",
    "    usecols=[\"Message-ID\", \"Date\", \"content\"],\n",
    "    parse_dates=[\"Date\"],\n",
    ").rename(columns={\"Message-ID\": \"document\", \"Date\": \"date\"})\n",
    "df[\"document\"] = df[\"document\"].str[1:23].str.replace(\".\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"data/enron/metadata.csv\", parse_dates=[\"date\"])\n",
    "metadata = metadata[(metadata[\"date\"] < pd.to_datetime(config[\"start_date\"] + pd.Timedelta(\"1W\"))) & (metadata[\"date\"] >= pd.to_datetime(config[\"start_date\"]))]\n",
    "df = pd.merge(metadata, df, on=\"document\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "docs = df[\"content\"].dropna().astype(str).str.lower().to_list()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english') + [\"forwarded\", \"am\", \"pm\", \"from\", \"cc\", \"bcc\", \"subject\", \"forward\", \"mailreply\", \"com\", \"org\", \"gmt\", \"mail\"] + [\"ect\"]\n",
    "docs = [[token for token in doc if token  not in sw] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 12667\n",
      "Number of documents: 3601\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 5000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,  \n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -2.4252.\n",
      "[([(0.011342138, 'enron'),\n",
      "   (0.0063620745, 'wa'),\n",
      "   (0.005222481, 'one'),\n",
      "   (0.0047698887, 'would'),\n",
      "   (0.00473559, 'market'),\n",
      "   (0.0039872197, 'get'),\n",
      "   (0.0038127557, 'ha'),\n",
      "   (0.0035334574, 'power'),\n",
      "   (0.003469485, 'year'),\n",
      "   (0.003440364, 'game'),\n",
      "   (0.0033879702, 'time'),\n",
      "   (0.0033303588, 'said'),\n",
      "   (0.0032698067, 'going'),\n",
      "   (0.0032264045, 'california'),\n",
      "   (0.0031395338, 'corp'),\n",
      "   (0.0031207353, 'take'),\n",
      "   (0.0030830982, 'price'),\n",
      "   (0.0029907145, 'new'),\n",
      "   (0.0029593937, 'go'),\n",
      "   (0.0028403064, 'week')],\n",
      "  -1.6865139274739742),\n",
      " ([(0.04180374, 'hou'),\n",
      "   (0.030349214, 'enron'),\n",
      "   (0.010210962, 'tana'),\n",
      "   (0.008373747, 'know'),\n",
      "   (0.008209231, 'tana_jones'),\n",
      "   (0.008055073, 'jones'),\n",
      "   (0.006586608, 'corp'),\n",
      "   (0.0053297137, 'john'),\n",
      "   (0.005285224, 'ee'),\n",
      "   (0.005111249, 'wa'),\n",
      "   (0.0050600986, 'mark'),\n",
      "   (0.0049245115, 'would'),\n",
      "   (0.0045485166, 'let'),\n",
      "   (0.004533672, 'group'),\n",
      "   (0.004404223, 'deal'),\n",
      "   (0.004117032, 'weather'),\n",
      "   (0.00410552, 'david'),\n",
      "   (0.0038965065, 'change'),\n",
      "   (0.0038505003, 'carol'),\n",
      "   (0.0036515302, 'thanks')],\n",
      "  -1.7863610954563829),\n",
      " ([(0.017378232, 'power'),\n",
      "   (0.014660664, 'energy'),\n",
      "   (0.011391222, 'transmission'),\n",
      "   (0.010049291, 'service'),\n",
      "   (0.009302618, 'price'),\n",
      "   (0.008102445, 'company'),\n",
      "   (0.007793289, 'edison'),\n",
      "   (0.007500296, 'city'),\n",
      "   (0.007278123, 'aep'),\n",
      "   (0.007060335, 'gas'),\n",
      "   (0.006966027, 'wa'),\n",
      "   (0.0053747897, 'would'),\n",
      "   (0.0052753845, 'california'),\n",
      "   (0.0050992104, 'new'),\n",
      "   (0.0050642584, 'id'),\n",
      "   (0.0050234348, 'contract'),\n",
      "   (0.004844643, 'utility'),\n",
      "   (0.004732578, 'tax'),\n",
      "   (0.004697661, 'agreement'),\n",
      "   (0.0044978335, 'provide')],\n",
      "  -2.025692221833846),\n",
      " ([(0.0955424, 'enron'),\n",
      "   (0.07424463, 'hou'),\n",
      "   (0.027758108, 'corp'),\n",
      "   (0.0069565326, 'please'),\n",
      "   (0.0060948073, 'john'),\n",
      "   (0.006073007, 'gco'),\n",
      "   (0.0054069795, 'mike'),\n",
      "   (0.0052786684, 'mark'),\n",
      "   (0.0052471342, 'communication'),\n",
      "   (0.003923861, 'thanks'),\n",
      "   (0.0038653894, 'business'),\n",
      "   (0.0037801622, 'na'),\n",
      "   (0.0035145665, 'robert'),\n",
      "   (0.0034411456, 'david'),\n",
      "   (0.0030326548, 'need'),\n",
      "   (0.0028151444, 'lon'),\n",
      "   (0.002761325, 'ee'),\n",
      "   (0.0027428465, 'kay'),\n",
      "   (0.0026544693, 'attached'),\n",
      "   (0.0026185478, 'change')],\n",
      "  -2.075729694652074),\n",
      " ([(0.10357767, 'enron'),\n",
      "   (0.030775819, 'et'),\n",
      "   (0.0064126044, 'hou'),\n",
      "   (0.0062386724, 'business'),\n",
      "   (0.006160748, 'meeting'),\n",
      "   (0.0053223916, 'corp'),\n",
      "   (0.005081563, 'august'),\n",
      "   (0.00428383, 'new'),\n",
      "   (0.0041369917, 'john'),\n",
      "   (0.004110197, 'please'),\n",
      "   (0.0040018964, 'thanks'),\n",
      "   (0.0038413964, 'mark'),\n",
      "   (0.0038299924, 'market'),\n",
      "   (0.0038137003, 'ee'),\n",
      "   (0.0035783907, 'wa'),\n",
      "   (0.0034534044, 'kay'),\n",
      "   (0.0033428275, 'power'),\n",
      "   (0.0033331097, 'fgt'),\n",
      "   (0.0032628493, 'global'),\n",
      "   (0.0032137004, 'north')],\n",
      "  -2.4280852164282396),\n",
      " ([(0.019364627, 'please'),\n",
      "   (0.012393662, 'enron'),\n",
      "   (0.0114774695, 'doc'),\n",
      "   (0.008600041, 'attached'),\n",
      "   (0.008480398, 'message'),\n",
      "   (0.0071753794, 'agreement'),\n",
      "   (0.006782312, 'hou'),\n",
      "   (0.006578662, 'know'),\n",
      "   (0.0065739634, 'thanks'),\n",
      "   (0.0060727964, 'let'),\n",
      "   (0.0052803573, 'information'),\n",
      "   (0.0052437205, 'may'),\n",
      "   (0.005049383, 'intended'),\n",
      "   (0.004766289, 'issue'),\n",
      "   (0.0046388237, 'question'),\n",
      "   (0.004625382, 'document'),\n",
      "   (0.0044195508, 'schedule'),\n",
      "   (0.00429106, 'need'),\n",
      "   (0.0042061144, 'meter'),\n",
      "   (0.0041388376, 'claim')],\n",
      "  -2.4510141073127945),\n",
      " ([(0.015720062, 'price'),\n",
      "   (0.013676509, 'iso'),\n",
      "   (0.008441784, 'market'),\n",
      "   (0.007829859, 'enron'),\n",
      "   (0.0076771793, 'option'),\n",
      "   (0.007437509, 'cap'),\n",
      "   (0.0073433546, 'energy'),\n",
      "   (0.006631545, 'model'),\n",
      "   (0.0064768707, 'risk'),\n",
      "   (0.006316629, 'chapter'),\n",
      "   (0.005658386, 'ee'),\n",
      "   (0.005610172, 'ferc'),\n",
      "   (0.0053827297, 'hou'),\n",
      "   (0.00488666, 'management'),\n",
      "   (0.0046485425, 'ha'),\n",
      "   (0.004611024, 'also'),\n",
      "   (0.004236256, 'vince'),\n",
      "   (0.00416603, 'order'),\n",
      "   (0.004098359, 'purchase'),\n",
      "   (0.0040263557, 'price_cap')],\n",
      "  -2.8172172694888733),\n",
      " ([(0.012672487, 'enron'),\n",
      "   (0.0076905848, 'would'),\n",
      "   (0.0075105056, 'see'),\n",
      "   (0.007432154, 'engage'),\n",
      "   (0.0071073542, 'file'),\n",
      "   (0.006548578, 'please'),\n",
      "   (0.006097218, 'attached'),\n",
      "   (0.005934589, 'hou'),\n",
      "   (0.0058521293, 'carol'),\n",
      "   (0.0056830575, 'isda'),\n",
      "   (0.0056717168, 'know'),\n",
      "   (0.005625438, 'like'),\n",
      "   (0.0055420883, 'st'),\n",
      "   (0.0055379514, 'guaranty'),\n",
      "   (0.005211986, 'carol_st'),\n",
      "   (0.005211281, 'clair'),\n",
      "   (0.0051878607, 'u'),\n",
      "   (0.0051277126, 'thanks'),\n",
      "   (0.00512723, 'see_attached'),\n",
      "   (0.0049692155, 'wa')],\n",
      "  -2.821854428705209),\n",
      " ([(0.008119058, 'enron'),\n",
      "   (0.0075156665, 'would'),\n",
      "   (0.0066876756, 'gas'),\n",
      "   (0.0056674643, 'url'),\n",
      "   (0.0045841634, 'wa'),\n",
      "   (0.0045634545, 'please'),\n",
      "   (0.004299103, 'contract'),\n",
      "   (0.0042822342, 'time'),\n",
      "   (0.003938925, 'thanks'),\n",
      "   (0.003759325, 'wildhorse'),\n",
      "   (0.0037029297, 'information'),\n",
      "   (0.0036491484, 'report'),\n",
      "   (0.003638469, 'ha'),\n",
      "   (0.003435237, 'deal'),\n",
      "   (0.003368198, 'market'),\n",
      "   (0.0033257753, 'energy'),\n",
      "   (0.0032879743, 'meeting'),\n",
      "   (0.003221012, 'next'),\n",
      "   (0.003181771, 'position'),\n",
      "   (0.0031688185, 'u')],\n",
      "  -2.907435785188366),\n",
      " ([(0.040751312, 'ee'),\n",
      "   (0.03005026, 'hou'),\n",
      "   (0.012981278, 'ee_ee'),\n",
      "   (0.01212743, 'enron'),\n",
      "   (0.010804262, 'vince'),\n",
      "   (0.0062640165, 'please'),\n",
      "   (0.006166176, 'issue'),\n",
      "   (0.006109318, 'would'),\n",
      "   (0.0060074627, 'market'),\n",
      "   (0.0057956083, 'kaminski'),\n",
      "   (0.0057402807, 'shirley'),\n",
      "   (0.00561125, 'vince_kaminski'),\n",
      "   (0.004982257, 'sheila'),\n",
      "   (0.0046886518, 'ha'),\n",
      "   (0.0046515577, 'need'),\n",
      "   (0.0044977926, 'anita'),\n",
      "   (0.004350753, 'kean'),\n",
      "   (0.0041879076, 'know'),\n",
      "   (0.0038858443, 'david'),\n",
      "   (0.0038102295, 'ferc')],\n",
      "  -3.251729413003512)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-857717b52ba84eb9aa233e7181dd30d7\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-857717b52ba84eb9aa233e7181dd30d7\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-857717b52ba84eb9aa233e7181dd30d7\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9444bfac9819f5b9b2f6fc912b6a5dbf\"}, \"mark\": {\"type\": \"bar\", \"color\": \"purple\"}, \"encoding\": {\"x\": {\"field\": \"year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"content\", \"type\": \"quantitative\"}}, \"width\": 900, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9444bfac9819f5b9b2f6fc912b6a5dbf\": [{\"year\": \"1980-01\", \"content\": 522}, {\"year\": \"1986-04\", \"content\": 1}, {\"year\": \"1986-05\", \"content\": 1}, {\"year\": \"1997-01\", \"content\": 1}, {\"year\": \"1997-03\", \"content\": 39}, {\"year\": \"1997-04\", \"content\": 36}, {\"year\": \"1997-05\", \"content\": 32}, {\"year\": \"1997-06\", \"content\": 64}, {\"year\": \"1997-07\", \"content\": 56}, {\"year\": \"1997-08\", \"content\": 77}, {\"year\": \"1997-09\", \"content\": 72}, {\"year\": \"1997-10\", \"content\": 28}, {\"year\": \"1997-11\", \"content\": 32}, {\"year\": \"1998-01\", \"content\": 4}, {\"year\": \"1998-05\", \"content\": 1}, {\"year\": \"1998-09\", \"content\": 1}, {\"year\": \"1998-10\", \"content\": 8}, {\"year\": \"1998-11\", \"content\": 56}, {\"year\": \"1998-12\", \"content\": 107}, {\"year\": \"1999-01\", \"content\": 128}, {\"year\": \"1999-02\", \"content\": 91}, {\"year\": \"1999-03\", \"content\": 112}, {\"year\": \"1999-04\", \"content\": 97}, {\"year\": \"1999-05\", \"content\": 662}, {\"year\": \"1999-06\", \"content\": 645}, {\"year\": \"1999-07\", \"content\": 857}, {\"year\": \"1999-08\", \"content\": 1054}, {\"year\": \"1999-09\", \"content\": 1239}, {\"year\": \"1999-10\", \"content\": 1390}, {\"year\": \"1999-11\", \"content\": 1310}, {\"year\": \"1999-12\", \"content\": 3542}, {\"year\": \"2000-01\", \"content\": 6351}, {\"year\": \"2000-02\", \"content\": 7182}, {\"year\": \"2000-03\", \"content\": 8736}, {\"year\": \"2000-04\", \"content\": 8599}, {\"year\": \"2000-05\", \"content\": 10229}, {\"year\": \"2000-06\", \"content\": 13672}, {\"year\": \"2000-07\", \"content\": 13598}, {\"year\": \"2000-08\", \"content\": 18984}, {\"year\": \"2000-09\", \"content\": 19895}, {\"year\": \"2000-10\", \"content\": 24675}, {\"year\": \"2000-11\", \"content\": 32437}, {\"year\": \"2000-12\", \"content\": 31301}, {\"year\": \"2001-01\", \"content\": 23957}, {\"year\": \"2001-02\", \"content\": 23092}, {\"year\": \"2001-03\", \"content\": 28453}, {\"year\": \"2001-04\", \"content\": 35716}, {\"year\": \"2001-05\", \"content\": 35589}, {\"year\": \"2001-06\", \"content\": 18728}, {\"year\": \"2001-07\", \"content\": 10181}, {\"year\": \"2001-08\", \"content\": 8892}, {\"year\": \"2001-09\", \"content\": 10831}, {\"year\": \"2001-10\", \"content\": 37060}, {\"year\": \"2001-11\", \"content\": 28543}, {\"year\": \"2001-12\", \"content\": 11348}, {\"year\": \"2002-01\", \"content\": 21014}, {\"year\": \"2002-02\", \"content\": 8169}, {\"year\": \"2002-03\", \"content\": 3438}, {\"year\": \"2002-04\", \"content\": 1157}, {\"year\": \"2002-05\", \"content\": 903}, {\"year\": \"2002-06\", \"content\": 917}, {\"year\": \"2002-07\", \"content\": 244}, {\"year\": \"2002-09\", \"content\": 6}, {\"year\": \"2002-10\", \"content\": 1}, {\"year\": \"2002-12\", \"content\": 1}, {\"year\": \"2004-02\", \"content\": 70}, {\"year\": \"2005-12\", \"content\": 1}, {\"year\": \"2007-02\", \"content\": 1}, {\"year\": \"2012-11\", \"content\": 2}, {\"year\": \"2020-12\", \"content\": 2}, {\"year\": \"2024-05\", \"content\": 1}, {\"year\": \"2043-12\", \"content\": 1}, {\"year\": \"2044-01\", \"content\": 3}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"year\"] = df[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "disp = df.groupby(\"year\", as_index=False)['content'].count()\n",
    "alt.Chart(disp).mark_bar(color=\"purple\").encode(x=\"year:O\", y=\"content\").properties(width=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"date\"].dt.year.isin(range(1997, 2005))]\n",
    "df[[\"category\", \"date\"]].to_csv(\"data/enron/metadata.csv\", index=False)\n",
    "df = df.set_index([\"category\", \"date\"])\n",
    "df[\"content\"] = (\n",
    "    df[\"content\"]\n",
    "    .replace(\"‘\", \"'\")\n",
    "    .replace(\"’\", \"'\")\n",
    "    .replace(\"“\", '\"')\n",
    "    .replace(\"”\", '\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element_code</th>\n",
       "      <th>element</th>\n",
       "      <th>global_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>621888</td>\n",
       "      <td>ect</td>\n",
       "      <td>0.024199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>660150</td>\n",
       "      <td>enron</td>\n",
       "      <td>0.018691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1309014</td>\n",
       "      <td>pm</td>\n",
       "      <td>0.010263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1598346</td>\n",
       "      <td>subject</td>\n",
       "      <td>0.008128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1307315</td>\n",
       "      <td>please</td>\n",
       "      <td>0.006271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>747168</td>\n",
       "      <td>forwarded</td>\n",
       "      <td>0.005793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1651950</td>\n",
       "      <td>thanks</td>\n",
       "      <td>0.005455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1395143</td>\n",
       "      <td>re</td>\n",
       "      <td>0.004406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1066043</td>\n",
       "      <td>mark</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>543905</td>\n",
       "      <td>dc</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>170082</td>\n",
       "      <td>&gt; &gt;</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>857517</td>\n",
       "      <td>houston</td>\n",
       "      <td>0.003457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1095936</td>\n",
       "      <td>message</td>\n",
       "      <td>0.003282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>963464</td>\n",
       "      <td>june</td>\n",
       "      <td>0.002803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>739074</td>\n",
       "      <td>fly</td>\n",
       "      <td>0.002441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>459513</td>\n",
       "      <td>company plane</td>\n",
       "      <td>0.002325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>626657</td>\n",
       "      <td>ees</td>\n",
       "      <td>0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>360755</td>\n",
       "      <td>bryson</td>\n",
       "      <td>0.002194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1082314</td>\n",
       "      <td>may</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1817132</td>\n",
       "      <td>your</td>\n",
       "      <td>0.002099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    element_code        element  global_weight\n",
       "0         621888            ect       0.024199\n",
       "1         660150          enron       0.018691\n",
       "2        1309014             pm       0.010263\n",
       "3        1598346        subject       0.008128\n",
       "4        1307315         please       0.006271\n",
       "5         747168      forwarded       0.005793\n",
       "6        1651950         thanks       0.005455\n",
       "7        1395143             re       0.004406\n",
       "8        1066043           mark       0.004364\n",
       "9         543905             dc       0.003800\n",
       "10        170082            > >       0.003537\n",
       "11        857517        houston       0.003457\n",
       "12       1095936        message       0.003282\n",
       "13        963464           june       0.002803\n",
       "14        739074            fly       0.002441\n",
       "15        459513  company plane       0.002325\n",
       "16        626657            ees       0.002231\n",
       "17        360755         bryson       0.002194\n",
       "18       1082314            may       0.002142\n",
       "19       1817132           your       0.002099"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results/enron/dvr.csv\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [x for x in pd.date_range(end=pd.datetime.today(), periods=1800)]\n",
    "counts = [x for x in np.random.randint(0, 10000, size=1800)]\n",
    "df = pd.DataFrame({'dates': dates, 'counts': counts}).set_index('dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i, it in enumerate(df.iterrows()):\n",
    "    if i <= 500000:\n",
    "        continue\n",
    "    index, row = it\n",
    "    npc = TextBlob(str(row[\"content\"])).np_counts\n",
    "    if npc == {}:\n",
    "        continue\n",
    "    adf = pd.DataFrame.from_records(list(npc.items()), columns=[\"element\", \"frequency_in_category\"])\n",
    "    adf[\"category\"] = index[0]\n",
    "    l.append(adf)\n",
    "concated = pd.concat(l)\n",
    "concated.to_csv(f\"data/enron/np_freq/{i}.csv\")\n",
    "l = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('lpa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "602d5137c5051c7ad1050fca0bcc77f792b5903d7dd0c5290bd7ccb1dc571dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
